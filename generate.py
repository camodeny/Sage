import os
import json
import google.generativeai as genai
from huggingface_hub import InferenceClient
import io
from PIL import Image
import base64
import requests
from bs4 import BeautifulSoup
from google.generativeai.types.generation_types import GenerateContentResponse
import datetime

# Wolfram App ID
# 58XQ9G-989TUAVAUW


gemini_key =  os.environ['google_llm_api']
hugging_face_key = os.environ['hugging_face_api']
brave_key = os.environ['brave_api']

genai.configure(api_key=gemini_key)


determine_type_system = """You are a helpful assistant that takes in an INPUT and creates a chain of steps to complete the request. You have a series of tools that you can use to complete the request. You have access to the following tools:

APIs
1. Brave_Search: A tool that allows you to search the web for information. You can use this tool to look up information about a topic. You can also include a web_search_prompt, which will use Gemeni 1.5 Flash to summarize the page data with a custom prompt. A default prompt will be provided if a custom prompt is not needed. (input: text, output: json)
2. Wolfram_Alpha: A tool that allows you to use all of the calculators from Wolfram Alpha (input: json, output: json)

MODELS
1. LLaMA 3.1 70B: A tool that allows you to do advanced coding, technical discussions, and problem-solving (input: text, json; output: text)
2. Nous Hermes 2 Mixtral 8x7B DPO: a tool that allows you to do logical reasoning, in-depth analysis. (input: text, json; output: text)
3. Gemini Flash: a tool that allows you to do general chatting, simple coding tasks, creative writing, and image analysis, best for multi-language support. (it cannot create images, just analyze them). (input: text, audio, image, json; output: text)
4. FLUX.1-dev: a tool that allows you to do image generation tasks. No prompt can be provided for this model in situations where the prompt needs to be generated from the tool chain history, a model will create a prompt from the chain history. If a prompt can be generated by you then it is preferred (input: text; output: image)

Nous Hermes 2 Mixtral 8x7B DPO and Gemini flash are provided the current date and time. You do not need to use another tool for that information.

you can combine these tools to create a chain of steps to complete the request. Models require a prompt to descibe the task and what you want the model to do. If a step should be displayed to the user set display_response to true. For example if an image is generated and used this should be displayed or if a web search is used this should always be displayed to cite sources, but summarizing an image for another model should not be displayed.

You can use the following format:
{
    "request": "The request you want to complete",
    "tools": [
        {
            "input": ["text", "json", "image", "audio"]
            "tool_name": "The name of the tool you want to use",
            "tool_purpuse": "The purpose of the tool you want to use",
            (REQUIRED FOR ALL MODELS EXCEPT FLUX) "tool_prompt": "The prompt for the model",
            (optional) "web_search_prompt": "The prompt for the web_search tool",
            "output": ["text", "json", "image", "audio"],
            "display_response": true/false
        }
    ]
}

Handling Image Inputs:
Only Gemini Flash can accept image inputs directly. If a request involves image analysis or manipulation that requires the output from another model (like LLaMA or Nous Hermes), you must first:
Pass the image to Gemini Flash for analysis or description.
Use the text description generated by Gemini as input for the other models.

an example of a request and a tool chain:

Request: "Create a detailed report on the current state of AI research, including breakthroughs in neural networks and any relevant academic papers."

Tool Chain: {
    "request": "Create a detailed report on the current state of AI research, including breakthroughs in neural networks and any relevant academic papers.",
    "tools": [
        {
            "input": ["text"],
            "tool_name": "Brave_Search",
            "tool_purpose": "Search for recent breakthroughs in neural networks and any relevant academic papers.",
            "output": ["json"],
            "display_response": true
        },
        {
            "input": ["json"],
            "tool_name": "Nous Hermes 2 Mixtral 8x7B DPO",
            "tool_purpose": "Analyze the search results from Brave_Search to extract key breakthroughs and summarize academic papers.",
            "tool_prompt": "Please review these search results and extract key information about recent neural network breakthroughs and summarize important academic papers in the field.",
            "output": ["text"],
            "display_response": false
        },
        {
            "input": ["text"],
            "tool_name": "Gemini Flash",
            "tool_purpose": "Provide an in-depth technical explanation of the breakthroughs in neural networks.",
            "tool_prompt": "Using the information provided, write a detailed report on the recent advancements in neural networks and any key concepts introduced.",
            "output": ["text"],
            "display_response": true
        }
    ]
}

Prompts should be used to fufill the purpose of the tool. If it is to respond back in conversational style, forward on the original input.

DO NOT RESPOND TO THE ORIGINAL INPUT. ONLY BUILD THE CHAIN OF STEPS.

Respond in the language you are spoken to in. Make sure all tool purposes are in the target language too.
"""

create_chain_model = genai.GenerativeModel("gemini-1.5-flash", system_instruction=determine_type_system)
def determineRequestType(inputs):

    model_input = []
    for input in inputs['chat_chain']:
        print(input)
        if input['type'] == 'text':
            if input['role'] == 'user':
                model_input.append({'role': input['role'], 'parts': [{input['type']: input['content']}]})
            else:
                model_input.append({'role': 'model', 'parts': [{input['type']: input['content']}]})
        elif input['type'] == 'image':
            if input['role'] == 'user':
                model_input.append({
                    'role': 'user',
                    'parts': [
                        input['content']
                    ]
                })
            else:
                model_input.append({
                    'role': 'model',
                    'parts': [
                        input['content']
                    ]
                })
    model_input[-1]['parts'][0]["text"] = 'INPUT: ' + model_input[-1]['parts'][0]["text"]
    
    response = create_chain_model.generate_content(model_input)
    return response.text



#steps = determineRequestType(PROMPT)

#if "```json" in steps:
#    steps = steps.split("```json\n")[1].split("\n```")[0]

#steps = json.loads(steps)










######
#    Gemini 1.5 Flash
######


def gemini(step, inputs, stream):
    now = datetime.datetime.now()
    current_datetime = now.strftime("%Y-%m-%d %H:%M:%S")
    model = genai.GenerativeModel("gemini-1.5-flash", system_instruction="You are a helpful assistant. When asked about your name or who you are you will respond with Sage. You are capable of web searching, math, image generation, math, and complex chained tasts.\n\nKnowledge cutoff: December 2023\nCurrent date:"+current_datetime)
    prompt = step["tool_prompt"]

    model_input = []
    for input in inputs:
        if input['type'] == 'text':
            if input['role'] == 'user':
                model_input.append({'role': input['role'], 'parts': [{input['type']: input['content']}]})
            else:
                model_input.append({'role': 'model', 'parts': [{input['type']: input['content']}]})
        elif input['type'] == 'image':
            if input['role'] == 'user':
                model_input.append({
                    'role': 'user',
                    'parts': [
                        input['content']
                    ]
                })
            else:
                model_input.append({
                    'role': 'model',
                    'parts': [
                        input['content']
                    ]
                })

    model_input.append({'role': 'user', 'parts': [{"text":prompt}]})

    if not stream:
        response = model.generate_content(model_input)
        return {'role': 'assistant', 'content': response.text, "type": "text"}, {'role': 'user', 'content': prompt, "type": "text"}, None
    else:
        return None, {'role': 'user', 'content': prompt, "type": "text"}, model.generate_content(model_input, stream=True)







######
#    LLaMA 3.1 70B
######


def llama_70(step, inputs, stream):
    model = InferenceClient(
        "meta-llama/Meta-Llama-3.1-70B-Instruct",
        token=hugging_face_key,
    )
    prompt = step["tool_prompt"]
    now = datetime.datetime.now()
    current_datetime = now.strftime("%Y-%m-%d %H:%M:%S")
    built_input = [{"role": "system", "content": "You are a helpful assistant. When asked about your name or who you are you will respond with Sage. You are capable of web searching, math, image generation, math, and complex chained tasts.\n\nKnowledge cutoff: December 2023\nCurrent date:"+current_datetime}]
    for input in inputs:
        if input['type'] != 'image':
            built_input.append({'role': input['role'], 'content': input['content']})

    built_input.append({'role': 'user', 'content': prompt})

    if not stream:
        response = model.chat_completion(messages=built_input,max_tokens=30000).choices[0].message.content
        return {'role': 'assistant', 'content': response, "type": "text"}, {'role': 'user', 'content': prompt, "type": "text"}, None
    else:
        return None, {'role': 'user', 'content': prompt, "type": "text"}, model.chat_completion(messages=built_input,max_tokens=30000, stream=True)









######
#    Nous Hermes 2 Mixtral 8x7B DPO
######


def nous_hermes(step, inputs, stream):
    model = InferenceClient(
        "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
        token=hugging_face_key,
    )
    prompt = step["tool_prompt"]
    now = datetime.datetime.now()
    current_datetime = now.strftime("%Y-%m-%d %H:%M:%S")
    built_input = [{"role": "system", "content": "You are a helpful assistant. When asked about your name or who you are you will respond with Sage. You are capable of web searching, math, image generation, math, and complex chained tasts.\n\nKnowledge cutoff: December 2023\nCurrent date:"+current_datetime}]
    for input in inputs:
        if input['type'] != 'image':
            built_input.append({'role': input['role'], 'content': input['content']})

    built_input.append({'role': 'user', 'content': prompt})

    if not stream:
        response = model.chat_completion(messages=built_input,max_tokens=30000).choices[0].message.content
        return {'role': 'assistant', 'content': response, "type": "text"}, {'role': 'user', 'content': prompt, "type": "text"}, None
    else:
        return None, {'role': 'user', 'content': prompt, "type": "text"}, model.chat_completion(messages=built_input,max_tokens=30000, stream=True)







######
#    Flux.1-dev
######

def buildPrompt(step, inputs):
    model = genai.GenerativeModel("gemini-1.5-flash", system_instruction="You are a helpful assistant. You are given a chat history and an image generation purpose. Your job is to create a prompt for an image generation model. Please respond in the following format: {\"prompt\": \"<prompt>\"}")

    model_input = []
    for input in inputs:
        if input['type'] == 'text':
            if input['role'] == 'user':
                model_input.append({'role': input['role'], 'parts': [{input['type']: input['content']}]})
            else:
                model_input.append({'role': 'model', 'parts': [{input['type']: input['content']}]})
        elif input['type'] == 'image':
            model_input.append({
                'role': 'model',
                'parts': [
                    input['content']
                ]
            })

    model_input.append({'role': 'user', 'parts': [{"text":"PURPOSE: "+step['tool_purpose']}]})

    response = model.generate_content(model_input).text
    if "```json" in response:
        response = response.split("```json\n")[1].split("\n```")[0]

    response = json.loads(response)
    return response['prompt']

def flux(step, inputs):
    model = InferenceClient(
        "black-forest-labs/FLUX.1-dev",
        token=hugging_face_key
    )

    if not "tool_prompt" in step:
        prompt = buildPrompt(step, inputs)
    elif step["tool_prompt"] == "":
        prompt = buildPrompt(step, inputs)
    else:
        prompt = step["tool_prompt"]

    response = model.text_to_image(prompt)
    buffered = io.BytesIO()
    response.save(buffered, format="PNG")
    img_bytes = buffered.getvalue()
    img_base64_bytes = base64.b64encode(img_bytes)
    img_base64_string = "data:image/png;base64," + img_base64_bytes.decode('utf-8')

    return {'role': 'assistant', 'content': img_base64_string, "type": "image"}, {'role': 'user', 'content': prompt, "type": "text"}







######
#    Brave Search
######

def removeExtraSpaces(input_string):
    result = []
    space_found = False

    for char in input_string:
        if char == ' ':
            if not space_found:
                result.append(char)
            space_found = True
        else:
            result.append(char)
            space_found = False

    return ''.join(result)

def fetchPage(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
    }
    session = requests.Session()
    page_data = session.get(url, timeout=5, headers=headers)
    page_data.raise_for_status()
    soup = BeautifulSoup(page_data.text, 'html.parser')
    if soup.html:
        for script in soup(["script", "head", "style", "meta", "title"]):
          script.extract()
        page_text_final = soup.html.get_text().replace('\n', ' ')
        return removeExtraSpaces(page_text_final)
    else:
        return "THIS IS A BOT DETECTION PAGE"

def checkIfBotPage(page_data):
    model = genai.GenerativeModel("gemini-1.5-flash")
    prompt = "Is this a bot detection page? If it is, respond with 'true', otherwise respond with 'false'.\n\n Respond in the following format: \n\n{\"bot_page\": true/false}\n\n" + page_data

    response = model.generate_content(prompt).text
    if "```json" in response:
        response = response.split("```json\n")[1].split("\n```")[0]
    try:
        json.loads(response)
    except:
        print(response)
    return json.loads(response)['bot_page']

def summarizePages(pages, prompt=None):
    full_pages_string = ' '.join(pages)
    model = genai.GenerativeModel("gemini-1.5-flash")

    if not prompt:
        prompt = "Summarize the following text into a concise and informative summary. Make sure to include all relevant information. The summary you provide will be used as context for another LLM\n\n"

    prompt += "The information below is real time and was just gathered.\n\n"

    prompt = prompt + full_pages_string

    response = model.generate_content(prompt).text
    return response

def brave(step, inputs):
    model = genai.GenerativeModel("gemini-1.5-flash")

    model_input = []
    for input in inputs:
        if input['type'] == 'text':
            if input['role'] == 'user':
                model_input.append({'role': input['role'], 'parts': [{input['type']: input['content']}]})
            else:
                model_input.append({'role': 'model', 'parts': [{input['type']: input['content']}]})
        elif input['type'] == 'image':
            model_input.append({
                'role': 'model',
                'parts': [
                    input['content']
                ]
            })

    model_input.append({'role': 'user', 'parts': [{"text":"please create a search query that satisfies the following criteria:\n"+step["tool_purpose"]+'\n\n please respond in the format {"query": "search_query"}'}]})

    response = model.generate_content(model_input).text

    if "```json" in response:
        response = response.split("```json\n")[1].split("\n```")[0]

    response = json.loads(response)

    url = "https://api.search.brave.com/res/v1/web/search"
    query = "brave+search"

    headers = {
        "Accept": "application/json",
        "Accept-Encoding": "gzip",
        "X-Subscription-Token": brave_key
    }

    params = {
        "q": response['query']
    }

    response = requests.get(url, headers=headers, params=params)
    responses_list = response.json()

    good_pages = []
    good_urls = []
    try:
        tmp = responses_list['web']
    except KeyError:
        print(responses_list)
    for response in responses_list['web']['results']:
        if len(good_pages) < 3:
            try:
                page_data = fetchPage(response['url'])

                if not checkIfBotPage(page_data):
                    good_pages.append(page_data)
                    good_urls.append({"title":response['title'], "url":response['url']})
            except requests.exceptions.HTTPError:
                pass
            except requests.exceptions.ReadTimeout:
                pass
        else:
            break
    summary = "REAL TIME WEB SEARCH RESULTS:\n\n"
    if "web_search_prompt" in step:
        summary += summarizePages(good_pages, step["web_search_prompt"])
    else:
        summary += summarizePages(good_pages)

    return {'role': 'assistant', 'content': summary, "type": "text"}, good_urls


















def buildChain(steps, history):
    tool_chain = steps['tools']
    step_count = len(tool_chain)
    current_step = 0
    for tool in tool_chain:
        current_step += 1
        prompt = None
        stream = None


        if current_step < step_count:
            stream_status = False
        elif current_step == step_count and tool['tool_name'] == "FLUX.1-dev":
            stream_status = False
        else:
            stream_status = True

        tool['stream'] = stream_status

        yield "tool", None, None, tool

        if tool['tool_name'] == "Gemini Flash":
            response, prompt, stream = gemini(tool, history, stream_status)
        elif tool['tool_name'] == "LLaMA 3.1 70B":
            response, prompt, stream = llama_70(tool, history, stream_status)
        elif tool['tool_name'] == "Nous Hermes 2 Mixtral 8x7B DPO":
            response, prompt, stream = nous_hermes(tool, history, stream_status)
        elif tool['tool_name'] == "FLUX.1-dev":
            response, prompt = flux(tool, history)
        elif tool['tool_name'] == "Brave_Search":
            response, urls = brave(tool, history)
            tool['sources'] = urls
        if prompt:
            history.append(prompt)
        if response:
            history.append(response)
            tool['tool_response'] = response['content']

        yield "content", stream_status, stream, tool





def createPlan(inputs):
    steps = determineRequestType(inputs)
    
    if "```json" in steps:
        steps = steps.split("```json\n")[1].split("\n```")[0]
    try:
        steps = json.loads(steps)
    except:
        print(steps)

    return steps